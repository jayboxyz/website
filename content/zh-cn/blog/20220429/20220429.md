+++ 
title = "一次openLooKeng的Pipeline之旅：上游算子处理能力加快会使得下游算子也加快么？"
date = "2022-04-29"
tags = [ "openLooKeng", "算子下推"]
archives = "2022-04"
author = "廖登宏"
description = "一次openLooKeng的Pipeline之旅：上游算子处理能力加快会使得下游算子也加快么？"
+++

## <p align="left">1 背景</p>

在大数据处理的软件中，如openLooKeng或presto，数据处理往往是以pipeline的方式，通过将`data`在不同的`Operator`之间流转，从而实现对数据的处理。

以处理以下sql为例：`select  sum(ss_sales_price), sum(ss_list_price) from store_sales_partition_1 where ss_sold_date_sk > 0 group by  ss_hdemo_sk`;

以上sql会形成三个`stage`，在读数据的`source stage`主要包含`Scan`、`Agg(partial)`、`FilterAndProject`、`PartitionOutput`四个算子，数据处理的大部分时间都在这个`stage`（因为`partial agg`后，数据量大大减少），一种典型的pipeline如下：

<img align="center" src='/zh-cn/blog/20220429/001.jpg' />

## <p align="left">2 问题</p>

**那么在pipeline的模式下，TableScanOperator的处理速度加快后，是否其他算子的处理速度也会加快呢？**

为了验证这个猜想，我们构造了以下两种场景：

1. 正常的`table scan`，读取ORC文件；

2. 使用`ORC Cache`，将读取后生成的block缓存在内存中，下次读取无需读取原始文件。

以下为测试数据：

<<test result.xlsx>>


<img align="center" src='/zh-cn/blog/20220429/002.jpg' />

## <p align="left">3 结论</p>

**结论：从以上数据可知，通过ORC Cache后，Scan的CPU时间减少了约一半，E2E的时间也因为Scan算子的减少而相应减少，但是其他算子的处理速度却不会因此而提升。**

**理论分析：A算子从上游算子获取的数据是一定的，无论上游算子是在1s内还是10s内给算子A，那么算子A执行`addinput`和`getoutput`进行数据处理的时间就是一定的，因为假设10s某些时间上游算子没有喂给A数据，那么A是不参与运算的，即既不记录CPU时间也不计`wall time`。**

以下为`scan`和`agg`处理速度的截图，从图中直观看到`scan`处理速度加快，而下游的`hashagg`几乎没有变化：

<img align="center" src='/zh-cn/blog/20220429/003.jpg' />

## <p align="left">几个发散的思考</p>

### ORC cache

开启`hetu.split-cache-map.enabled=true`后并执行`cache table`，那么`table cache`后的`split`会`cache`，会使用`cache`的`split`调度策略，而非默认的调度策略，此时会突破`node-scheduler.max-splits-per-node`的限制，这看起来更像是个bug。

`hive.orc.row-data.block.cache.enabled`开启后，`split`和`row data`都会`cache`，且`cache`的`data`是解压后的`block`，同时`filetail`、`footer`等元数据信息没有开启，因此仍然会去读元数据信息，但是感觉已经没有必要了，因为读这些信息并不能过滤数据，仍然会使用cache的数据？----这是一个bug？

### 调度影响

当`node-scheduler.max-splits-per-node`使用默认值100时，由于单节点处理的数据量很小，因而`queued`和`running`的`split`最大为100，需要分多批次调度，而`hetu.split-cache-map.enabled=true`后`queue`和`running`的`splits`无限大，**为何分批次调度的影响这么大，值得进一步分析。**

###  文件系统Cache

`row data cache`后减少的这一半时间主要是ORC读取上来后的解压缩时间，因为实际数据已经缓存在文件系统`cache`中，因而数据从内核态拷贝到用户态是很快的。
 
在最早的分析中，我们发现算子级别的`wall`和`cpu time`一样，而端到端时间差异几倍，怀疑是`IO wait`，但是实际上通过`IOstat`等命令看，没有`io wait`，甚至没有`io`读写，发现是因为文件只有700M，基本全缓存到文件系统`cache`了，从而进一步理解了文件系统`cache`。

同时我们怀疑是否有异步线程的时间没有统计到`driver`的`process`中，实际上也没有这种情况发生，因为哪怕是异步线程处理，那么在cpu图上一定能看到痕迹。

通过以下两篇文章，能较好的了解异步IO：

[译]

Linux 异步 I/O 框架 io_uring：基本原理、程序示例与性能压测（2020）
来自 <https://arthurchiao.art/blog/intro-to-io-uring-zh/> 

Java NIO浅析
来自 <https://tech.meituan.com/2016/11/04/nio.html> 


---

如果您有任何疑问或建议，欢迎在社区代码仓内提Issue；也欢迎加小助手微信(openLooKengoss)，进入专属技术交流群。

社区代码仓 

<https://gitee.com/openlookeng>

<https://github.com/openlookeng>


openLooKeng，让大数据更简单！